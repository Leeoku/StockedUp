{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package conll2000 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "#!pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz\n",
    "# !pip install spacy\n",
    "# !python3 -m spacy download en\n",
    "# !pip install -U textblob\n",
    "# !python3 -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import requests, json\n",
    "import pandas as pd, numpy as np\n",
    "from api import ocr_key\n",
    "from collections import OrderedDict\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from textblob import TextBlob\n",
    "\n",
    "nlp = spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7793\n",
      "Unique enteries present in the data : 918\n"
     ]
    }
   ],
   "source": [
    "ks = pd.read_excel('food.xlsx')\n",
    "\n",
    "# ks.head(10)\n",
    "mst_common = []\n",
    "shrt = ks['Long_Desc'].tolist()\n",
    "\n",
    "# for item in shrt:\n",
    "#     item = item.split(',')\n",
    "#     for i in item: \n",
    "#         mst_common[i] = mst_common.get(i, 0) + 1\n",
    "    \n",
    "# sort_items = sorted(mst_common.items(), key=lambda x: x[1], reverse=True)\n",
    "3\n",
    "# print(len(mst_common))\n",
    "\n",
    "# sort_items\n",
    "\n",
    "for item in shrt : \n",
    "    item = item.split(',')\n",
    "    mst_common.append(item[0].lower())\n",
    "    \n",
    "\n",
    "mst_common = set(mst_common)\n",
    "print(\"Unique enteries present in the data :\" ,len(mst_common))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def FoodDatabase():\n",
    "#     df = pd.read_excel(\"food.xlsx\", usecols=\"B:C\")\n",
    "#     food_group_map = {\n",
    "#         100: \"dairy\",\n",
    "#         1800: \"grain\",\n",
    "#         2000: \"grain\",\n",
    "#         500: \"meat\",\n",
    "#         700: \"meat\",\n",
    "#         1000: \"meat\",\n",
    "#         1300: \"meat\",\n",
    "#         1500: \"meat\",\n",
    "#         1700: \"meat\",\n",
    "#         900: \"fruit_veg\",\n",
    "#         1100: \"fruit_veg\",\n",
    "#         1600: \"fruit_veg\",\n",
    "#     }\n",
    "#     df[\"Food Group\"] = df[\"FdGrp_Cd\"].map(food_group_map)\n",
    "#     df_split = df[\"Long_Desc\"].str.split(\",\", n=1, expand=True)\n",
    "#     df_split.columns = [\"Food Name\", \"Food Name Detail\"]\n",
    "#     df = pd.concat([df, df_split], axis=1)\n",
    "#     df = df.groupby(\"Food Group\")[\"Food Name\"].unique()\n",
    "#     dairy = df[\"dairy\"]\n",
    "#     grain = df[\"grain\"]\n",
    "#     meat = df[\"meat\"]\n",
    "#     fruit_veg = df[\"fruit_veg\"]\n",
    "#     print(df)\n",
    "    \n",
    "#     return dairy, grain, meat, fruit_veg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_excel(\"food.xlsx\", usecols=\"B:C\")\n",
    "# print(df.head())\n",
    "\n",
    "# food_group_map = {\n",
    "#     100: \"dairy\",\n",
    "#     1800: \"grain\",\n",
    "#     2000: \"grain\",\n",
    "#     500: \"meat\",\n",
    "#     700: \"meat\",\n",
    "#     1000: \"meat\",\n",
    "#     1300: \"meat\",\n",
    "#     1500: \"meat\",\n",
    "#     1700: \"meat\",\n",
    "#     900: \"fruit_veg\",\n",
    "#     1100: \"fruit_veg\",\n",
    "#     1600: \"fruit_veg\",\n",
    "# }\n",
    "\n",
    "# df[\"Food Group\"] = df[\"FdGrp_Cd\"].map(food_group_map)\n",
    "\n",
    "# df_split = df[\"Long_Desc\"].str.split(\",\", n=1, expand=True)\n",
    "# df_split.columns = [\"Food Name\", \"Food Name Detail\"]\n",
    "\n",
    "\n",
    "# df = pd.concat([df, df_split], axis=1)\n",
    "\n",
    "# display(df)\n",
    "# df = df.groupby(\"Food Group\")[\"Food Name\"].unique()\n",
    "# dairy = df[\"dairy\"]\n",
    "# grain = df[\"grain\"]\n",
    "# meat = df[\"meat\"]\n",
    "# fruit_veg = df[\"fruit_veg\"]\n",
    "\n",
    "\n",
    "# print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OCR SPACE IMPORTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food Group\n",
      "dairy        [Butter, Butter oil, Cheese, Cheese food, Chee...\n",
      "fruit_veg    [Acerola, Acerola juice, Apples, Apple juice, ...\n",
      "grain        [Bagels, Biscuits, Bread, Cake, Cookies, Puff ...\n",
      "meat         [Chicken, Canada Goose, Duck, Goose, Guinea he...\n",
      "Name: Food Name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def ocr_space_file(\n",
    "    filename, overlay=False, api_key=ocr_key, language=\"eng\", istable=True, scale=True\n",
    "):\n",
    "    \"\"\" OCR.space API request with local file.\n",
    "        Python3.5 - not tested on 2.7\n",
    "\n",
    "    :param filename: Your file path & name.\n",
    "    :param overlay: Is OCR.space overlay required in your response.\n",
    "                    Defaults to False.\n",
    "    :param api_key: OCR.space API key.\n",
    "                    Defaults to 'helloworld'.\n",
    "    :param language: Language code to be used in OCR.\n",
    "                    List of available language codes can be found on https://ocr.space/OCRAPI\n",
    "                    Defaults to 'en'.\n",
    "    :return: Result in JSON format.\n",
    "    :param istable: parses output as a table\n",
    "    \"\"\"\n",
    "\n",
    "    payload = {\n",
    "        \"isOverlayRequired\": overlay,\n",
    "        \"apikey\": api_key,\n",
    "        \"language\": language,\n",
    "        \"istable\": istable,\n",
    "    }\n",
    "    with open(filename, \"rb\") as f:\n",
    "        r = requests.post(\n",
    "            \"https://api.ocr.space/parse/image\", files={filename: f}, data=payload,\n",
    "        )\n",
    "    return r.content.decode()\n",
    "  \n",
    "def ocr_space_url(\n",
    "    url, overlay=False, api_key=ocr_key, language=\"eng\", isTable=True, scale=True\n",
    "):\n",
    "    \"\"\" OCR.space API request with remote file.\n",
    "        Python3.5 - not tested on 2.7\n",
    "\n",
    "    :param url: Image url.\n",
    "    :param overlay: Is OCR.space overlay required in your response.\n",
    "                    Defaults to False.\n",
    "    :param api_key: OCR.space API key.\n",
    "                    Defaults to 'helloworld'.\n",
    "    :param language: Language code to be used in OCR.\n",
    "                    List of available language codes can be found on https://ocr.space/OCRAPI\n",
    "                    Defaults to 'en'.\n",
    "    :return: Result in JSON format.\n",
    "    :param isTable: parses output as a table\n",
    "    \"\"\"\n",
    "\n",
    "    payload = {\n",
    "        \"url\": url,\n",
    "        \"isOverlayRequired\": overlay,\n",
    "        \"apikey\": api_key,\n",
    "        \"language\": language,\n",
    "        \"isTable\": isTable,\n",
    "    }\n",
    "    r = requests.post(\"https://api.ocr.space/parse/image\", data=payload,)\n",
    "    # return r.content.decode()\n",
    "\n",
    "    # Change this return in source code to return JSON object, not string\n",
    "    return r.json()\n",
    "\n",
    "\n",
    "# API response and obtaing \"LineText\"\n",
    "def parse():\n",
    "    data = ocr_space_url(\"https://ocr.space/Content/Images/receipt-ocr-original.jpg\")\n",
    "    lines = data.get(\"ParsedResults\")[0][\"TextOverlay\"][\"Lines\"]\n",
    "    words = [line.get(\"LineText\").lower() for line in lines]\n",
    "    return words\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parse()\n",
    "    FoodDatabase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the FoodDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the excel sheet and obtain the target food groups. Note not food group codes used\n",
    "def food_database():\n",
    "    df = pd.read_excel(\"food.xlsx\", usecols=\"B:C\")\n",
    "    food_group_map = {\n",
    "        100: \"dairy\",\n",
    "        1800: \"grain\",\n",
    "        2000: \"grain\",\n",
    "        500: \"meat\",\n",
    "        700: \"meat\",\n",
    "        1000: \"meat\",\n",
    "        1300: \"meat\",\n",
    "        1500: \"meat\",\n",
    "        1700: \"meat\",\n",
    "        900: \"fruit_veg\",\n",
    "        1100: \"fruit_veg\",\n",
    "        1600: \"fruit_veg\",\n",
    "    }\n",
    "    df[\"Food Group\"] = df[\"FdGrp_Cd\"].map(food_group_map)\n",
    "    df_split = df[\"Long_Desc\"].str.split(\",\", n=1, expand=True)\n",
    "    df_split.columns = [\"Food Name\", \"Food Name Detail\"]\n",
    "    df = pd.concat([df, df_split], axis=1)\n",
    "    df = df.groupby(\"Food Group\")[\"Food Name\"].unique()\n",
    "    dairy = df[\"dairy\"]\n",
    "    grain = df[\"grain\"]\n",
    "    meat = df[\"meat\"]\n",
    "    fruit_veg = df[\"fruit_veg\"]\n",
    "    food_groups = np.concatenate((dairy, meat, grain, fruit_veg))\n",
    "    # print(type(food_groups))\n",
    "    #return dairy, grain, meat, fruit_veg\n",
    "    return food_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_or_int(string):\n",
    "    if string.isdigit():\n",
    "        return True\n",
    "    elif string.replace('.','',1).isdigit() and string.count('.') < 2:\n",
    "        return True\n",
    "    else:\n",
    "        return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(all_words):\n",
    "    text_doc = nlp(all_words)\n",
    "    \n",
    "    # removing stop words or punctuation \n",
    "    text_doc2 = [] \n",
    "    for token in text_doc : \n",
    "        if(not token.is_stop and len(token.text) > 2 and not float_or_int(token.text)): \n",
    "#             print(\"token length == \",len(token), \" token text len ==\", len(token.text), \" token text -\", token.text)\n",
    "            text_doc2.append(token.lemma_)\n",
    "  \n",
    "    words_string = ' '.join(text_doc2)\n",
    "    \n",
    "    text_blob_object = TextBlob(words_string)\n",
    "    \n",
    "    singular_string = ' '.join(text_blob_object.words.singularize())\n",
    "    \n",
    "#     plural_string = ' '.join(text_blob_object.words.pluralize())\n",
    "    \n",
    "    return(singular_string,words_string)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patter_match():\n",
    "\n",
    "    \n",
    "    nlp = spacy.load('en')\n",
    "    matcher = PhraseMatcher(nlp.vocab, attr = 'LOWER')\n",
    "    \n",
    "    parsed_words = parse()\n",
    "'''\n",
    "    parsed_words = ['walmart', 'save money. live better.', \n",
    "    '( 330 ) 339 -', '3991', 'manager diana earnest',\n",
    "     '231 bluebell dr sw', 'new philadelphia oh 44663',\n",
    "      '02115 009044 44', '01301', 'pet toy', 'floppy puppy', \n",
    "      'sssupreme s', 'z . 5 squeak', 'munchy dmbel', 'dog treat', \n",
    "      'ped pch 1', 'ped pch 1', 'coupon 23100', '1--inymd smores',\n",
    "       'french drsng', '3 oranges', 'baby carrots', 'collards',\n",
    "        'calzone', 'mm rvw mnt', 'stkobrlplabl', 'stkobrlplabl', \n",
    "        'stko sunflwr', 'stko sunflwr', 'stko sunflwr',\n",
    "        'stko sunflwr', 'bling beads', 'great value', \n",
    "        'lipton', 'dry dog', 'tax', 'us debit', '004747571658', \n",
    "        '004747514846', '070060332153', '084699803238', '068113108796',\n",
    "         '007119013654', '002310011802', '002310011802', '052310037000', '088491226837', '004132100655', '001466835001', '003338366602', '1', '000000004614k1', '005208362080 f', '003399105848', '001558679414', '001558679414', '001558679410', '001558679410', '001558679410', '001558679410', '076594060699', '007874203191 f', '001200011224 f', '002310011035', '1', 'subtotal', '6.750', 'total', 'visa tend', '5', '12 .', '9166', '1.97', '1.97', '4.97', '5.92', '3.77', '2.92', '0.50', '0.50', '1.00-0', '3.98', '1.98', '.47', '1.48', '1.24', '2.50', '19.77', '1.97', '1.97', '0.97', '0.97', '0.97', '0.97', '0.97', '9.97', '4.48', '44', '93.62', '4.59', '98.21', '98.21', '1', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'o', 'o', 'o', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'o', 'x', 'x', 'o', 'approval # 572868', 'ref # 720900544961', 'trans id', '387209239650894', 'validation - 87hs', 'payment service', 'aid a0000000980840', 'tc 51319ca81dcz2bc7', 'terminal # sc010764', '*signature ver ified', '07/28/17', '02 : 39 : 48', 'change due', '# items sold 25', '0443 0223 1059 8001 5140', 'low'\n",
    "    , 'ices you can trust .', 'every', '0.00', 'day .', '07/28/17', \n",
    "    '02 : 39 : 48']\n",
    "''' \n",
    "    \n",
    "    words_string = ' '.join(parsed_words)\n",
    "    \n",
    "    (single,plural) = filter_words(words_string)\n",
    "    \n",
    "    \n",
    "    text_doc_singular = nlp(single)\n",
    "    text_doc_plural   = nlp(plural)\n",
    "\n",
    "\n",
    "    food_groups = food_database()\n",
    "\n",
    "    # Building the intial list of keywords we want to match the parsed items aganist\n",
    "    patterns = [nlp(text) for text in food_groups]\n",
    "    matcher.add(\"Food Matcher\", None, *patterns)    \n",
    "    matches = matcher(text_doc_singular)\n",
    "    \n",
    "    results = []\n",
    "    # printing all th matches, TerminologyList is the name of the our patter matcher\n",
    "    for i in range(len(matches)):    \n",
    "        match_id, start, end = matches [0]\n",
    "#         results.append((nlp.vocab.strings[match_id], text_doc_singular[start:end]))\n",
    "        results.append(text_doc_singular[start:end])\n",
    "\n",
    "    matches = matcher(text_doc_plural)\n",
    "    \n",
    "\n",
    "    for i in range(len(matches)):    \n",
    "        match_id, start, end = matches [0]\n",
    "#         results.append((nlp.vocab.strings[match_id],text_doc_plural[start:end]))\n",
    "        results.append(text_doc_plural[start:end])\n",
    "    \n",
    "    results = dict((i, results.count(i)) for i in results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************\n",
      "walmart save money live well manager diana earnest bluebell new philadelphium pet toy floppy puppy sssupreme squeak munchy dmbel dog treat pe pch ped pch coupon inymd smore french drsng orange baby carrot collard calzone rvw mnt stkobrlplabl stkobrlplabl stko sunflwr stko sunflwr stko sunflwr stko sunflwr bling bead great value lipton dry dog tax debit 000000004614k1 subtotal total visa tend approval ref tran validation 87h payment service aid a0000000980840 51319ca81dcz2bc7 terminal sc010764 signature ver ifie 07/28/17 change item sell low ice trust day 07/28/17\n",
      "************\n",
      "walmart save money live well manager diana earnest bluebell new philadelphia pet toy floppy puppy sssupreme squeak munchy dmbel dog treat pe pch ped pch coupon -inymd smore french drsng oranges baby carrots collards calzone rvw mnt stkobrlplabl stkobrlplabl stko sunflwr stko sunflwr stko sunflwr stko sunflwr bling beads great value lipton dry dog tax debit 000000004614k1 subtotal total visa tend approval ref trans validation 87hs payment service aid a0000000980840 51319ca81dcz2bc7 terminal sc010764 signature ver ifie 07/28/17 change item sell low ice trust day 07/28/17\n",
      "************\n",
      "{carrot: 1, oranges: 3}\n"
     ]
    }
   ],
   "source": [
    "patter_match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
